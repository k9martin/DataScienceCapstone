---
title: "Milestone Report"
author: "Francisco Martín"
date: "November, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This is my Milestone report for Data Science Capstone course. Here I will process data and 
create three important files for the project. This three files will contain 2-grams, 3-grams 
and 4-grams for my future text predictor.


## Exploratory analysis

First of all I will make some exploratory analysis of the data. Before that, data should be 
downloaded (if needed) and loaded:


``` {r libraries, echo=FALSE}
suppressPackageStartupMessages(library(stringi)) 
suppressPackageStartupMessages(library(NLP))
suppressPackageStartupMessages(library(openNLP)) 
suppressPackageStartupMessages(library(tm)) 
suppressPackageStartupMessages(library(rJava)) 
suppressPackageStartupMessages(library(RWeka)) 
suppressPackageStartupMessages(library(RWekajars)) 
suppressPackageStartupMessages(library(SnowballC)) 
suppressPackageStartupMessages(library(ggplot2))
```

```{r download_data}

url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
fileName <- "Coursera-SwiftKey.zip"
path <- file.path(getwd(),fileName)

if (!file.exists(path)){
        download.file(url,path)
}

folderData <- "final"
pathFolder <- file.path(getwd(),folderData)

if(!file.exists(pathFolder)){
        unzip(path)
}

englishFolder <- file.path(pathFolder, "en_US")
blogsPath <- file.path(englishFolder,"en_US.blogs.txt")
newsPath <- file.path(englishFolder,"en_US.news.txt")
twitterPath <-file.path(englishFolder,"en_US.twitter.txt")

# I will only work on english data.

twitterReaded <- readLines(twitterPath)
blogsReaded <- readLines(blogsPath)
newsReaded <- readLines(newsPath)

```

Once data is loaded, I will check some features of the data:

``` {r features_data}
# Size of the files:

sizeBlog <- file.size(blogsPath)/(1024^2)
sizeNews <- file.size(newsPath)/(1024^2)
sizeTwitter <- file.size(twitterPath)/(1024^2)

# Number of lines:

linesTwitter <- length(twitterReaded)
linesBlog <- length(blogsReaded)
linesNews <- length(newsReaded)

# Number of words:

wordsTwitter <- sum(stri_count_words(twitterReaded)) 
wordsBlogs <- sum(stri_count_words(blogsReaded)) 
wordsNews <- sum(stri_count_words(newsReaded))
```

Sizes are `r sizeTwitter`Mb for twitter set, `r sizeBlog`Mb for blog and `r sizeNews`Mb for news. 
Also, the number of lines are between `r linesTwitter` for twitter set and `r linesNews` for news, 
and number words are between `r wordsBlogs` for blogs and `r wordsNews` for news. 

Files are too big. If I want to make my predictor to work in, per example, a mobile phone, 
I need to use a much smaller subset. Just make it randomly with all three datasets and save it in a 
new file:

``` {r sample_file}
set.seed(1994)
sampleSize <- 15000
sampleTwitter <- sample(twitterReaded, size = sampleSize/3, replace = TRUE)
sampleBlogs <- sample(blogsReaded, size = sampleSize/3, replace = TRUE)
sampleNews <- sample(newsReaded, size = sampleSize/3, replace = TRUE)

smallSubset <- c(sampleTwitter, sampleBlogs, sampleNews)

smallSubsetPath <- file.path(englishFolder,"subsetNotClean.txt")
if (!file.exists(smallSubsetPath)){writeLines(smallSubset, smallSubsetPath)}
```

## Prepare data

Here I have created my subset for training my model. Before training it, I need to process all data 
and clean it, removing punctuations, emoticons, numbers, URLs, multiple whitespaces, and converting 
all letters to lowercase. For this I will use text mining package (tm):

``` {r clean_data}
# First I create a couple of functions for removing different things:
rvThings <- function(x,pattern){gsub(pattern,"",x)}
rvEmoticons <- function (x) {x <- iconv(x, "UTF-8", "ASCII", sub="")}

# And here I clean data
textTM <- Corpus(VectorSource(smallSubset))
textTM <- tm_map(textTM, removePunctuation)                        # remove punctuation
textTM <- tm_map(textTM, rvEmoticons)                               # remove URL
textTM <- tm_map(textTM, removeNumbers)                            # remove numbers
textTM <- tm_map(textTM, rvThings,"(f|ht)tp(s?)://(.*)[.][a-z]+")  # remove URL
textTM <- tm_map(textTM, stripWhitespace)                          # remove multiple whitespace
textTM <- tm_map(textTM, content_transformer(tolower))             # convert to lowercase

```

Also I have to remove profane words. Just tiping in Google "Profane word list" I found a list, and I 
will use it for removing this words:

``` {r profane_words}
urlProfane<- "http://www.cs.cmu.edu/~biglou/resources/bad-words.txt"
pathProfane <- file.path(englishFolder,"profaneWords.txt")
if(!file.exists(pathProfane)) {download.file(urlProfane,pathProfane)}

profaneReaded <- readLines(pathProfane)
textTM <- tm_map(textTM,removeWords,profaneReaded)

# Once data is clean, "rewrite" it as plain text
textTM <- tm_map(textTM,PlainTextDocument)
```

Here data is clean. Just proceed to make n-grams of it.

## N-Grams

N-Grams are set of N words which appears in a given dataset. In order to create a good text predictor, 
I will generate the set of 2-Grams, 3-Grams and 4-Grams of my subset, for comparing it to 3~1 final words 
of the future input of the text predictor. In order to do it, I will use RWeka package:

First 2-grams:

``` {r bigrams}
bigrams <- NGramTokenizer(textTM, Weka_control(min=2,max=2))
bigrams <- data.frame(table(bigrams)) # convert it to a data frame
bigrams <- bigrams[order(bigrams$Freq,decreasing = TRUE),] #Order in more common
names(bigrams) <- c("words","freq")  # Rename columns of bigrams dataframe
bigrams$words <- as.character(bigrams$words)
wordsBigrams <- strsplit(bigrams$words,split=" ")
bigrams <- transform(bigrams, word_1 = sapply(wordsBigrams,"[[",1),
                      word_2 = sapply(wordsBigrams,"[[",2))
bigrams2 <- data.frame(word_1 = bigrams$word_1, word_2 = bigrams$word_2, 
                       freq = bigrams$freq, stringsAsFactors=FALSE)
```

And I will proceed equaly to 3-grams and 4-grams:

``` {r triquadgrams}
trigrams <- NGramTokenizer(textTM, Weka_control(min=3,max=3))
trigrams <- data.frame(table(trigrams))
trigrams <- trigrams[order(trigrams$Freq,decreasing = TRUE),]
names(trigrams) <- c("words","freq")
trigrams$words <- as.character(trigrams$words)
wordsTrigrams <- strsplit(trigrams$words,split=" ")
trigrams <- transform(trigrams, word_1 = sapply(wordsTrigrams,"[[",1),
                     word_2 = sapply(wordsTrigrams,"[[",2),
                     word_3 = sapply(wordsTrigrams,"[[",3))
trigrams2 <- data.frame(word_1 = trigrams$word_1, word_2 = trigrams$word_2, 
                         word_3 = trigrams$word_3,
                         freq = trigrams$freq, stringsAsFactors=FALSE)


quadgrams <- NGramTokenizer(textTM, Weka_control(min=4,max=4))
quadgrams <- data.frame(table(quadgrams))
quadgrams <- quadgrams[order(quadgrams$Freq,decreasing = TRUE),]
names(quadgrams) <- c("words","freq")
quadgrams$words <- as.character(quadgrams$words)
wordsQuadgrams <- strsplit(quadgrams$words,split=" ")
quadgrams <- transform(quadgrams, word_1 = sapply(wordsQuadgrams,"[[",1),
                     word_2 = sapply(wordsQuadgrams,"[[",2),
                     word_3 = sapply(wordsQuadgrams,"[[",3),
                     word_4 = sapply(wordsQuadgrams,"[[",4))
quadgrams2 <- data.frame(word_1 = quadgrams$word_1,  word_2 = quadgrams$word_2, 
                         word_3 = quadgrams$word_3, word_4 = quadgrams$word_4,
                         freq = quadgrams$freq, stringsAsFactors=FALSE)

```

Now just save this N-Grams into r files, because they will be used in the text predictor:

``` {r save_files}
pathApp <- file.path(getwd(),"App")
if(!dir.exists(pathApp)){dir.create(pathApp)}


bigramsPath <- file.path(pathApp,"bigrams.RData")
trigramsPath <- file.path(pathApp,"trigrams.RData")
quadgramsPath <- file.path(pathApp,"quadgrams.RData")

if(!file.exists(bigramsPath)){
saveRDS(bigrams2, file = bigramsPath)
saveRDS(trigrams2, file = trigramsPath)
saveRDS(quadgrams2, file = quadgramsPath)}
```


# Some N-Grams plots

Here we have isolated most common N-grams. In order to see which they are, we can plot 
the 20 most common in bigrams, trigrams and quadgrams. Let`s see:

First, and using ggplot, I am going to define a plot function:

```{r makeplot}
top20 <- function(df, title) {
    df <- df[order(df$freq,decreasing = TRUE),]
    df <- df[1:20,]
    df$words <- factor(df$words, levels = df$words[order(-df$freq)])
    ggplot(df, aes(x = words, y = freq)) +
        geom_bar(stat = "identity", fill = "dodgerblue3", colour = "gray40") +
        labs(title = title, x="N-Gram", y="Count") +
        theme(axis.text.x = element_text(angle=60, size=12, hjust = 1),
              axis.title = element_text(size=14, face="bold"),
              plot.title = element_text(size=16, face="bold"))
}
```

And plot from bigrams, trigrans and quadgrams:

``` {r plotBigrams}
top20(bigrams, "20 Most Common Bigrams")
```
``` {r plotTrigrams}
top20(trigrams, "20 Most Common Trigrams")
```
``` {r plotQuadgrams}
top20(quadgrams, "20 Most Common Quadgrams")
```

